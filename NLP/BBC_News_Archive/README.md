# BBC News Archive Assessment

## Description
This project aims to dive deeper into the BBC News Classification Dataset, which contains 2,225 examples of news articles along with their respective labels. The project particularly focuses on:
- Tokenisation of text data
- Text classification using specialised layers such as Embedding and GlobalAveragePooling1D


## Techniques Covered
### Tokenisation
Tokenisation is the process of converting text into tokens before transforming it into vectors. It is crucial for any NLP problem.

### Text Classification using Embedding and GlobalAveragePooling1D layers
The use of Embedding and GlobalAveragePooling1D layers in the neural network for text classification.


## License
[MIT](https://choosealicense.com/licenses/mit/)
